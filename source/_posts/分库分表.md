---
title: 分库分表
date: 2021-12-26 15:30:56
tags: 
- 分库分表
categories:
- 分库分表
index_img: /img/mycat.png

---



















分库分表常见解决方案

<!--more-->



yum配置阿里镜像源 



> 1）下载repo文件
> wget http://mirrors.aliyun.com/repo/Centos-7.repo
>
> wget命令不存在 使用 yum进行安装即可  
>
> 
>
> yum install -y wget
>
> 
>
> 2）备份并替换系统的repo文件
>
> cp Centos-7.repo /etc/yum.repos.d/
> cd /etc/yum.repos.d/
> mv CentOS-Base.repo CentOS-Base.repo.bak
> mv Centos-7.repo CentOS-Base.repo
>
> 3）执行yum源更新命令
>
> yum clean all
> yum makecache
> yum update
>
> 配置完毕。

# 一、分库分表

## 1.1、切分数据库

水平切分 ：按照表的属性名进行切分

垂直切分 ：按照 userId 的哈希值或者一定规则将表中数据切分为不同的组成 

- 分布式事务问题
- 跨库join问题
- 多数据源的管理问题

> 针对多数据源的管理问题,主要有两种思路：
>
> 1. 客户端模式，在每个应用模块内，配置自己需要的数据源，直接访问数据库，在各模块内完成数据的整合
> 2. 中间代理模式，中间代理统一管理所有的数据源，数据库层对开发人员完全透明，开发人员无需关注拆分的细节

- 中间代理模式：MyCat
- 客户端模式:sharding-jdbc

## 1.2、读写分离

- 数据库分为读库和写库 写库中的数据更新要同步到读库

## 1.3、垂直切分

- 按照业务切分
- 每种业务一个数据库
- 不同业务之间，禁止跨库join联查(性能太差，一般通过服务来进行查询)

> 垂直切分
>
> 优点：
>
> - 拆分后业务清晰，拆分规则明确
> - 系统之间容易扩展和整合
> - 数据维护简单
>
> 缺点：
>
> - 部分业务表无法join，只能通过接口调用，提升了系统的复杂度
> - 跨库事务难以处理
> - 垂直切分后，某些业务数据过于庞大，仍然存在单体性能瓶颈

## 1.4、水平切分

- 将一张表的数据按照某种规则分到不同的数据库中
- 需确定分片的规则
- 使用分片字段查询时，可确定实体库，其它字段查询，查询所有表

> 水平切分
>
> 优点：
>
> - 解决了单库大数据，高并发的性能瓶颈
> - 拆分规则封装好，对应用端几乎透明，开发人员无需关心拆分细节
> - 提高了系统的稳定性和负载能力
>
> 缺点：
>
> - 拆分规则很难抽象
> - 分片事务一致性难以解决
> - 二次扩展时，数据迁移，维护难度大

## 1.5、分库分表选择

- 优先选择垂直切分
- 之后选择水平切分





# 二、MyCat

## 2.1、MyCat概述

- 对于DBA来说： MyCat就是MySql ， 而MyCat后面连接的mysql可以理解为mysql的存储引擎，比如：myisam，innodb等，mycat本身并不存储数据，数据都是存储在mycat后面连接的mysql上，数据的可靠性和事务都是mysql保证的
- 对于开发人员来说： mycat就是一个近似等于mysql的数据库服务，可以使用连接mysql的方式连接mycat。绝大多数情况，可以使用常用的orm框架连接mycat，但是对于分片的表，建议使用标准的sql语句，这样能够达到最佳的性能
- 对于架构师来说： mycat是一个强大的数据库中间件，不仅仅可以用作读写分离，分库分表，还可以用于容灾备份，云平台建设等，让架构具备更强的适应性和灵活性

> MyCat的应用场景
>
> MyCat发展到现在，使用的场景很丰富，常见的典型应用场景有
>
> - 单纯的读写分离，此时配置最为简单，支持读写分离，主从切换
> - 分库分表，对于超过1000w的表进行分片，最大支持1000亿的数据
> - 多租户应用，每个应用一个数据库，应用只连接MyCat，程序本身不需要改造
> - 代替Hbase，分析大数据

## 2.2、MyCat基本概念

MyCat是一个数据库的中间件，介于应用和数据库之间，是进行数据处理和交互的中间服务

### 1.逻辑库(Schema)

- 在实际的开发中，开发人员不需要知道数据库中间件的存在，开发人员只需要有数据库的概念就可以了，所以数据库中间件可以被看做一个或者多个数据库集群构成的逻辑库

### 2.逻辑表(table)

- 既然有逻辑库，就有逻辑表，对于应用系统来说，读写数据的表，就是逻辑表。而逻辑表中的数据则是被水平切分之后，分布在不同的分片库中。
- 假设用户库中有一张用户表，这个用户表就被称为逻辑表，而用户表又被水平切分为3个表，每一个逻辑表中都存储一部分用户数据。业务系统在对用户数据进行读写时，只需要操作逻辑表就可以了，后面的分片细节则由MyCat来进行操作，这对于业务开发人员来说是完全透明的。当然，有些表的数据量没有那么大，完全不需要进行分片，只在一个物理的数据库表中即可
- 凡是做的数据水平切分的表，都叫做分片表。而数据量较小，没有进行分片的表，叫做非分片表
- 在真实的业务系统中，往往存在着大量的字典表，这些表的数据基本上很少变动，比如：订单状态。查询的时候往往需要关联字段表去查询，比如：查询订单时，需要把订单状态关联查出，如果订单表做了分片，分布在不同的数据库中，而订单状态由于数据量小，没有做分片，那么我们查询的时候就要跨库关联查询订单状态，增加了不必要的麻烦，不如把订单状态冗余到所有的订单分片库中，这样关联查询就不需要跨库了，我们把这种通过数据冗余方式复制到所有分片库中的表 叫做==全局表==

### 3.分片节点(dataNode)

- 数据被切分后，一张大表被分到不同的分片数据库上面，每个分片表所在的数据库就叫做==分片节点==

### 4.分片主机(dataHost)

- 数据切分后，每一个分片节点不一定都会占用一个真正的物理主机，会存在多个分片节点在同一个物理主机上的情况，这些分片节点所在的主机就叫做==节点主机==，为了避免单节点并发数的限制，尽量将读写压力高的分片节点放在不同的节点主机上

### 5.分片规则(rule)

- 一个大表被拆分成多个分片表，就需要一定的规则，按照某种业务逻辑，将数据分到一个确定的分片当中，这个规则就叫做==分片规则==。数据切分选择合适的分片规则非常重要，这将影响到后续的数据处理难度，结合业务，选择合适的分片规则是一个艰难的，难以抉择的过程

### 6.全局序列号(sequence)

- 数据切分之后，数据库中的id怎么办，原来在一张表的时候，采用id自增，数据分布到多个数据可怎么办？比如向用户表插入数据，第一条记录插入了用户库1，id为1，第二条记录插入了用户库2，如果是自增，他的id也为1，这样就太混乱了，也无法确定一条数据的唯一标识了，我们需要借助外部机制来保证数据的唯一标识，这种保证数据唯一标识的机制，就是 ==全局序列号==

## 2.3、Mycat环境搭建

### 1.mysql环境搭建

> 三台虚拟机 两台安装mysql 一台安装Mycat，并修改配置文件

- 使用yum方式安装mysql (查看官网教程)
- 手动安装

 两台虚拟机配置好mysql环境

### 2.mycat环境搭建

解压mycat安装包到 /usr/local/mycat-1.6 目录下

> /conf 目录下下 修改配置文件
>
> server.xml 
>
> ```xml
> <user name="root" defaultAccount="true">
> 		<property name="password">123456</property>
> 		<property name="schemas">user</property>
> 		
> 		<!-- 表级 DML 权限设置 -->
> 		<!-- 		
> 		<privileges check="false">
> 			<schema name="TESTDB" dml="0110" >
> 				<table name="tb01" dml="0000"></table>
> 				<table name="tb02" dml="1111"></table>
> 			</schema>
> 		</privileges>		
> 		 -->
> 	</user>
> 
> 	<user name="user">
> 		<property name="password">user</property>
> 		<property name="schemas">user</property>
> 		<property name="readOnly">true</property>
> 	</user>
> ```
>
> schema.xml
>
> ```xml
> <!-- dataHost 配置 (对应mysql节点所在主机) -->
> <dataHost name="db101" maxCon="1000" minCon="10" balance="0"
> 			  writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
> 		<heartbeat>select user()</heartbeat>
> 		<!-- can have multi write hosts -->
> 		<writeHost host="M1" url="192.168.198.101:3306" user="root"
> 				   password="123456">
> 			<!-- can have multi read hosts -->
> 			<!--<readHost host="hostS2" url="192.168.1.200:3306" user="root" password="xxx" />-->
> 		</writeHost>
> 		
> 		<!--<writeHost host="hostS1" url="localhost:3316" user="root"
> 				   password="123456" />-->
> 		<!-- <writeHost host="hostM2" url="localhost:3316" user="root" password="123456"/> -->
> 	</dataHost>
> 	<dataHost name="db102" maxCon="1000" minCon="10" balance="0"
> 			  writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
> 		<heartbeat>select user()</heartbeat>
> 		<!-- can have multi write hosts -->
> 		<writeHost host="M1" url="192.168.198.102:3306" user="root"
> 				   password="123456">
> 			<!-- can have multi read hosts -->
> 			<!--<readHost host="hostS2" url="192.168.1.200:3306" user="root" password="xxx" />-->
> 		</writeHost>
> 		
> 		<!--<writeHost host="hostS1" url="localhost:3316" user="root"
> 				   password="123456" />-->
> 		<!-- <writeHost host="hostM2" url="localhost:3316" user="root" password="123456"/> -->
> 	</dataHost>
> 
> <!-- dataNode 配置-->
> 	<dataNode name="dn101" dataHost="db101" database="user_101" />
> 	<dataNode name="dn102" dataHost="db102" database="user_102" />
> <!-- 分片规则配置 -->
> <!-- schema name="user"  配置文件 中的name 要保持一致 -->
> <schema name="user" checkSQLschema="true" sqlMaxLimit="100">
> 		<!-- auto sharding by id (long) -->
> 		<table name="user" dataNode="dn101,dn102" rule="auto-sharding-long" />
> 	</schema>
> ```
>
>  

### 3.启动mycat

> ./bin/mycat console -- 可以查看mycat的日志信息
>
>  
>
> 启动报错 ： Caused by: io.mycat.config.util.ConfigException: Illegal table conf : table [ USER ] rule function [ rang-long ] partition size : 3 > table datanode size : 2, please make sure table datanode size = function partition size
>
>  
>
> 使用的分片算法 是将数据分为三个分片,但是现在配置的mysql数据库只有两个

> ```java
> # range start-end ,data node index
> # K=1000,M=10000.
> 0-500M=0
> 500M-1000M=1
> # 1000M-1500M=2
> ```
>
> 

### 4.测试 mycat 

> 连接mycat 在mycat服务器上进行数据的插入,
>
> ```sql
>  INSERT into `user` (id, user_name) VALUES (1, "tho")
> ```
>
> 记录物理保存在  198.101 服务器的mysql上
>
> 根据指定的分片规则 0 - 500w 数据都保存在 101 服务器上
>
> ```SQL
> INSERT into `user` (id, user_name) VALUES (6000000, "tho")
> ```
>
> 插入一条 600000w id 的数据 该数据会保存到 102服务器上

## 2.4、Mycat用户配置

### 1.server.xml 配置

- 配置MyCat 的用户名，密码，权限，Schema 等
- 如同给 mysql 新建用户一样
- 客户端连接MyCat 与 连接mysql 无异

### 2.直接使用 mysql8.0 cli 连接mycat

![image-20211223215410373](%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8.assets/image-20211223215410373.png)

> 因为mysql8.0 加密方式的变化 使用mysql8.0 cli 客户端可能无法直接连接mycat
>
> mysql8.0 cli 客户端 使用的是新的加密方式 
>
> 使用旧的加密方式
>
> `mysql -uroot -p8066 -h127.0.0.1 --default-auth=mysql_native_password ` 输入密码 连接mycat 

### 3.schema.xml配置

- 配置 dataHost(节点主机) ， 包括读host ， 写 host
- 配置 dataNode(数据节点) ， 指定到具体的数据库
- 配置 schema， 表名，数据节点，分片规则

> - balance 配置 ： 负载均衡类型 
> 	- 0 不开启读写分离
> 	- 1 和 2 读写均分配
> 	- 3 读落在 readHost 上
> - writeType 配置 ：写请求类型 0 落在第一个 writeHost 上 ；1 随机
> - sqlMaxLimit ：会拦截sql 自动加上 limit 条件 (sql语句本身有limit mycat的 limit 便不会再生效)
> - checkSQLschema：是否去掉 SQL 中的 Schema
> - table 标签：定义表
> 	- name属性：定义逻辑表的表名
> 	- dataNode属性：定义逻辑表的数据节点
> - rule属性：定义分片表的分配规则，必须与rule.xml 中的tableRule对应
> - ruleRequired属性：是否绑定分配规则，如果为 true 但是没有绑定分片规则,会报错

## 2.5、验证MyCat 配置

使用命令 `./bin/mycat start` 后台启动 mycat

> 使用 mycat 9066 端口可以 不停机查看 更新mycat中的配置

## 2.6、配置读写分离

schema.xml 配置文件修改

```xml
<writeHost host="M1" url="192.168.198.101:3306" user="root"
				   password="123456">
			<!-- can have multi read hosts -->
			<readHost host="S1" url="192.168.198.100:3306" user="root" password="123456" />
</writeHost>
```

198.100 服务器配置好 mysql 服务

```java
连接mycat的管理端口 9066 使用 `show @@help;` 查看所有命令
  使用 修改了 schema.xml 中 数据源 相关的内容,需要使用命令 reload @@config_all; 来使配置生效
```

## 2.7、Mysql主从配置

- 主配置 log-bin  指定文件的名字
- 主配置 server-id 默认为1
- 从配置 server-id 与主不能重复

### 1.主从配置文件配置

> 主mysql 配置文件 my.cnf 配置
>
> ```java
> # 配置mysql主从
> log-bin=tho_mysql
> server-id=1
> ```
>
> 



>  从mysql 配置文件 my.cnf 配置
>
> ```JAVA
> server-id=2
> ```
>
> 重启 主从数据库

### 2.创建备份用账号

> 主数据库创建备份账号并授权 REPLICATION SLAVE 

```sql
mysql> create user 'repl'@'%' identified by '123456'; 
(两个数据库都是mysql8.0 不需要考虑加密方式的问题,两个数据库都使用mysql8.0默认的加密方式)
Query OK, 0 rows affected (0.00 sec)

mysql> grant replication slave on *.* to 'repl'@'%';
Query OK, 0 rows affected (0.00 sec)

mysql> flush privileges;
Query OK, 0 rows affected (0.00 sec)

```

> 主 进行锁表 
>
> FLUSH TABLES WITH READ LOCK;

```sql
mysql> flush tables with read lock;
Query OK, 0 rows affected (0.01 sec)
```

> 主数据库 找到 log-bin 的位置 
>
> show master status;

```sql
mysql> show master status;
+------------------+----------+--------------+------------------+-------------------+
| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |
+------------------+----------+--------------+------------------+-------------------+
| tho_mysql.000001 |      739 |              |                  |                   |
+------------------+----------+--------------+------------------+-------------------+
1 row in set (0.00 sec)
```

> 主 备份数据 
>
> mysqldump --all-databases --master-data > dbdump.db

```sql
在执行mysqldump命令时候报如下错误：
[root@localhost ~]#  mysqldump -uroot -p******* --all-databases> all.sql   

mysqldump: Got error: 2002: Can't connect to local MySQL server through socket '/data/mysql/mysql.sock' (2) when trying to connect

找到正确的套接字的路径： 

[root@centos_7_100 mycat]# netstat -ln | grep mysql    
unix  2      [ ACC ]     STREAM     LISTENING     29653    /var/lib/mysql/mysql.sock
[root@centos_7_100 mycat]# 
[root@centos_7_100 mycat]# mysqldump --socket=/var/lib/mysql/mysql.sock --all-databases --master-data > dbdump.db -uroot -p
```

> 从 数据库 拷贝主数据库的备份文件
>
> scp root@192.168.198.100:~/dbdump.db .

```java
// 备份到从数据库
[root@centos_7_101 ~]# mysql < dbdump.db -uroot -p
Enter password: 

```

> 放开锁表 语句
>
> mysql> unlock tables;
> Query OK, 0 rows affected (0.01 sec)

### 3.配置主从

> 从库配置主库内容

```sql
change master to
master_host='192.168.198.100',
master_user='repl',
master_password='123456',
master_log_file='tho_mysql.000001',
master_log_pos=739;
```

> 从库 执行  start slave 开启主从同步
>
> 从库查看主从信息  show slave status; 

## 2.8、mycat常用分片规则

### 1.枚举分片

> 通过配置文件中配置可能的枚举id，自己配置分片，本规则适用于特定的场景，比如有些业务需要按照省市区来做保存，而全国的省市区是固定的，这类业务使用本条规则

### 2.取模分片

> 根据数据库某字段进行取模,按照取模的结果决定放到哪个数据库
>
> 后续扩展数据库,要按照当前数据库个数的n次方进行扩展

## 2.9、Mycat全局表

> 数据表的数据量较小(例如字典表等等),可以在每个分片数据库中都存放一份,这样不需要多次的联表查询
>
> ==type属性：global为全局表==，不指定为分片表

## 2.10、Mycat子表

> order(订单表) orderItem(订单明细表) 两表分库分表时最好放在同一个数据库中，减少跨库关联操作

- childTable标签，定义分片子表
- name属性，子表名称
- joinKey属性，标志子表中的列，用于与父表做关联(orderId)
- parentKey标签，标志父表中的列，与joinKey对应
- primaryKey属性，子表主键，同table标签
- needAddLimit属性，同table标签

> schema.xml 配置

```xml
<!-- 全局表配置 -->
<table name="province" dataNode="dn101,dn102" type="global" />
<!-- 子表配置 -->
<table name="user" dataNode="dn101,dn102" rule="auto-sharding-long" >
	<childTable name="order_item" joinKey="order_id" parentKey="id"/>
</table>
```

# 三、Mycat高可用

## 3.1、Mycat-HA-原理

> 避免Mycat成为系统中的单点

架构图

![image-20211229125856871](%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8.assets/image-20211229125856871.png)

## 3.2、Mycat-Haproxy

192.168.198.100 (mysql服务，mycat服务，HAproxy)

192.168.198.101 (mysql服务，mycat服务)

192.168.198.102 (mysql服务，HaProxy服务)

> 其中 100 和 101 两台主机是主从模式

### 1.安装HaProxy

> 使用 yum 命令方式安装

```shell
[root@centos_7_102 ~]# yum search haproxy
已加载插件：fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.aliyun.com
 * extras: mirrors.aliyun.com
 * updates: mirrors.aliyun.com
=========================================================== N/S matched: haproxy ============================================================
pcp-pmda-haproxy.x86_64 : Performance Co-Pilot (PCP) metrics for HAProxy
haproxy.x86_64 : TCP/HTTP proxy and load balancer for high availability environments

  名称和简介匹配 only，使用“search all”试试。
# haproxy比nginx支持的协议更多，nginx只支持http协议
# 安装haproxy
[root@centos_7_102 ~]# yum -y install haproxy.x86_64
```

> haproxy 配置文件所在目录 /etc/haproxy/haproxy.cfg

-  修改配置文件,通过tcp连接mycat 

```java
#---------------------------------------------------------------------
# Example configuration for a possible web application.  See the
# full configuration options online.
#
#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt
#
#---------------------------------------------------------------------

#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    # to have these messages end up in /var/log/haproxy.log you will
    # need to:
    #
    # 1) configure syslog to accept network log events.  This is done
    #    by adding the '-r' option to the SYSLOGD_OPTIONS in
    #    /etc/sysconfig/syslog
    #
    # 2) configure local2 events to go to the /var/log/haproxy.log
    #   file. A line like the following can be added to
    #   /etc/sysconfig/syslog
    #
    #    local2.*                       /var/log/haproxy.log
    #
    log         127.0.0.1 local2

    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    tcp
    log                     global
    option                  tcplog
    option                  dontlognull
    # option http-server-close
    # option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

#---------------------------------------------------------------------
# main frontend which proxys to the backends
#---------------------------------------------------------------------
frontend  main *:5000
    # acl url_static       path_beg       -i /static /images /javascript /stylesheets
    # acl url_static       path_end       -i .jpg .gif .png .css .js

    # use_backend static          if url_static
    default_backend             app

#---------------------------------------------------------------------
# static backend for serving up images, stylesheets and such
#---------------------------------------------------------------------
backend static
    balance     roundrobin
    server      static 127.0.0.1:4331 check

#---------------------------------------------------------------------
# round robin balancing between the various backends
#---------------------------------------------------------------------
backend app
    balance     roundrobin
    server  app1 192.168.198.100:8066 check
    server  app2 192.168.198.101:8066 check
    # server  app3 127.0.0.1:5003 check
    # server  app4 127.0.0.1:5004 check


```

### 2.启动HAProxy

```shell
[root@centos_7_102 ~]# haproxy -f /etc/haproxy/haproxy.cfg 
```

- 使用 navicat 测试 haproxy  5000 端口 (需要输入该主机上mycat的密码)

这样修改 haproxy 配置文件就可以消除警告信息

```xml
#---------------------------------------------------------------------
# Example configuration for a possible web application.  See the
# full configuration options online.
#
#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt
#
#---------------------------------------------------------------------

#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    # to have these messages end up in /var/log/haproxy.log you will
    # need to:
    #
    # 1) configure syslog to accept network log events.  This is done
    #    by adding the '-r' option to the SYSLOGD_OPTIONS in
    #    /etc/sysconfig/syslog
    #
    # 2) configure local2 events to go to the /var/log/haproxy.log
    #   file. A line like the following can be added to
    #   /etc/sysconfig/syslog
    #
    #    local2.*                       /var/log/haproxy.log
    #
    log         127.0.0.1 local2

    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    tcp
    log                     global
    option                  tcplog
    option                  dontlognull
    # option http-server-close
    # option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

#---------------------------------------------------------------------
# main frontend which proxys to the backends
#---------------------------------------------------------------------
frontend  main *:5000
    # acl url_static       path_beg       -i /static /images /javascript /stylesheets
    # acl url_static       path_end       -i .jpg .gif .png .css .js

    # use_backend static          if url_static
    default_backend             app

#---------------------------------------------------------------------
# static backend for serving up images, stylesheets and such
#---------------------------------------------------------------------
backend static
    balance     roundrobin
    server      static 127.0.0.1:4331 check

#---------------------------------------------------------------------
# round robin balancing between the various backends
#---------------------------------------------------------------------
backend app
    balance     roundrobin
    server  app1 192.168.198.100:8066 check
    server  app2 192.168.198.101:8066 check
    # server  app3 127.0.0.1:5003 check
    # server  app4 127.0.0.1:5004 check


```

## 3.3、keepAlived环境配置

### 1.安装keepAlived

> 通过 yum 方式安装 keepAlived

```shell
yum search keepalived
[root@centos_7_100 mycat]# yum install -y keepalived.x86_64
```

### 2.keepAlived配置文件

> 192.168.198.100   keepAlived 配置文件修改 (主节点)

```java
! Configuration File for keepalived

global_defs {
   notification_email {
     test@qq.com
   }
   # notification_email_from Alexandre.Cassen@firewall.loc
   # smtp_server 192.168.200.1
   # smtp_connect_timeout 30
   router_id node1
   # vrrp_skip_check_adv_addr
   # vrrp_strict
   # vrrp_garp_interval 0
   # vrrp_gna_interval 0
}

vrrp_instance VI_1 {
    state MASTER
    interface ens33
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.198.99
    }
}

virtual_server 192.168.198.99 6000 {
    delay_loop 6
    lb_algo rr
    lb_kind NAT
    persistence_timeout 50
    protocol TCP

    real_server 192.168.198.100 5000 {
        weight 1
        TCP_CHECK {
            connect_port 5000
			connect_timeout 10000
        }
    }
}

```

> 192.168.198.101  keepAlived 配置文件修改 (备节点)

```java
! Configuration File for keepalived

global_defs {
   notification_email {
     test@qq.com
   }
   # notification_email_from Alexandre.Cassen@firewall.loc
   # smtp_server 192.168.200.1
   # smtp_connect_timeout 30
   router_id node2
   # vrrp_skip_check_adv_addr
   # vrrp_strict
   # vrrp_garp_interval 0
   # vrrp_gna_interval 0
}

vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    virtual_router_id 51
    priority 99
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.198.99
    }
}

virtual_server 192.168.198.99 6000 {
    delay_loop 6
    lb_algo rr
    lb_kind NAT
    persistence_timeout 50
    protocol TCP

    real_server 192.168.198.101 5000 {
        weight 1
        TCP_CHECK {
            connect_port 5000
			connect_timeout 10000
        }
    }
}

```

### 3.启动keepAlived

> [root@centos_7_100 mycat]# keepalived -f /etc/keepalived/keepalived.conf 

### 4.小问题

> 这样配置完 keepAlived 在其中一台keepAlived 宕机的情况下会自动切换,但是 keepAlived 连接的服务宕机,keepAlived不会进行切换,会一直报错

- 解决办法：写一个脚本，后续连接的服务宕机之后,关掉当前的keepAlived服务，让keepAlived服务被动切换

- 需要使用 killall命令 该命令不存在则用yum 进行安装即可

```shell
[root@centos_7_100 mycat]# killall -0 haproxy
-bash: killall: 未找到命令
[root@centos_7_100 mycat]# yum search killall
已加载插件：fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.aliyun.com
 * extras: mirrors.aliyun.com
 * updates: mirrors.aliyun.com
========================================================================= 匹配：killall =========================================================================
psmisc.x86_64 : Utilities for managing processes on your system
[root@centos_7_100 mycat]# yum install -y psmisc.x86_64
```

```shell
[root@centos_7_100 mycat]# killall -0 haproxy
[root@centos_7_100 mycat]# echo $?
0
# killall 命令只是用来探测相关服务,不会终止掉相关服务
```

### 5.加入检测haproxy状态的脚本

keepAlived配置文件 /etc/keepalived/keepalived.conf 

192.168.198.100 keepAlived 配置文件

```java
! Configuration File for keepalived

global_defs {
   notification_email {
     test@qq.com
   }
   # notification_email_from Alexandre.Cassen@firewall.loc
   # smtp_server 192.168.200.1
   # smtp_connect_timeout 30
   router_id node1
   # vrrp_skip_check_adv_addr
   # vrrp_strict
   # vrrp_garp_interval 0
   # vrrp_gna_interval 0
}

vrrp_script chk_haproxy {
	script "killall -0 haproxy"
	interval  2
}


vrrp_instance VI_1 {
    state MASTER
    interface ens33
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.198.99
    }
	track_script {
		chk_haproxy
	}
}

virtual_server 192.168.198.99 6000 {
    delay_loop 6
    lb_algo rr
    lb_kind NAT
    persistence_timeout 50
    protocol TCP

    real_server 192.168.198.100 5000 {
        weight 1
        
    }
}
```



keepAlived配置文件 /etc/keepalived/keepalived.conf 

192.168.198.101 keepAlived 配置文件

```java
! Configuration File for keepalived

global_defs {
   notification_email {
     test@qq.com
   }
   # notification_email_from Alexandre.Cassen@firewall.loc
   # smtp_server 192.168.200.1
   # smtp_connect_timeout 30
   router_id node2
   # vrrp_skip_check_adv_addr
   # vrrp_strict
   # vrrp_garp_interval 0
   # vrrp_gna_interval 0
}
vrrp_script chk_haproxy {
	script "killall -0 haproxy"
	interval  2
}
vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    virtual_router_id 51
    priority 99
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
  	virtual_ipaddress {
        192.168.198.99
    }
    track_script {
			chk_haproxy
		}
}

virtual_server 192.168.198.99 6000 {
    delay_loop 6
    lb_algo rr
    lb_kind NAT
    persistence_timeout 50
    protocol TCP

    real_server 192.168.198.101 5000 {
        weight 1
        
    }
}

```

# 四、Springboot整合Mycat

## 4.1、基本环境

192.168.198.100    192.168.198.101 两台服务器是主从关系，100 是主节点 101 是从节点

> 对数据库进行分库分表
>
> 只拆分订单相关的三张表 order order_items order_status

## 4.2、分片规则

> 分片字段要选择 userId 这样一个用户的订单会进入到一个节点，不会分布到多个节点
>
> auto-sharding-long 只能用户整型变量



一致性Hash

![image-20220101114327186](%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8.assets/image-20220101114327186.png)

> 在 rule.xml 配置分片选择的字段
>
> <tableRule name="sharding-by-murmur">
> 		<rule>
> 			<columns>user_id</columns>
> 			<algorithm>murmur</algorithm>
> 		</rule>
> 	</tableRule>
>
> <function name="murmur"
> 		class="io.mycat.route.function.PartitionByMurmurHash">
> 		<property name="seed">0</property><!-- 默认是0 -->
> 		<property name="count">2</property><!-- 要分片的数据库节点数量，必须指定，否则没法分片 -->
> 		<property name="virtualBucketTimes">160</property><!-- 一个实际的数据库节点被映射为这么多虚拟节点，默认是160倍，也就是虚拟节点数是物理节点数的160倍 -->
> 		<!-- <property name="weightMapFile">weightMapFile</property> 节点的权重，没有指定权重的节点默认是1。以properties文件的格式填写，以从0开始到count-1的整数值也就是节点索引为key，以节点权重值为值。所有权重值必须是正整数，否则以1代替 -->
> 		<!-- <property name="bucketMapPath">/etc/mycat/bucketMapPath</property> 
> 			用于测试时观察各物理节点与虚拟节点的分布情况，如果指定了这个属性，会把虚拟节点的murmur hash值与物理节点的映射按行输出到这个文件，没有默认值，如果不指定，就不会输出任何东西 -->
> 	</function>



> mycat 中 作为选择数据库分片的字段不能进行更新
>
> 有子表配置时，先插入主表，才能进行子表的插入



# 五、Sharding-Jdbc使用

## 5.0、使用Mybatis-generator

### 1、pom.xml 中加入配置

```xml
<plugin>
  <groupId>org.mybatis.generator</groupId>
  <artifactId>mybatis-generator-maven-plugin</artifactId>
  <version>1.3.7</version>
  <dependencies>
    <dependency>	
      <groupId>mysql</groupId>
      <artifactId>mysql-connector-java</artifactId>
      <version>8.0.17</version>
    </dependency>
  </dependencies>
</plugin>
```

### 2、引入配置文件

> generatorConfig.xml
>
> 修改配置
>
> - connectionURL="jdbc:mysql://192.168.198.101:3306/shard_order?serverTimezone=Asia/Shanghai&amp;useSSL=false"
>
> - <javaModelGenerator targetPackage="com.tho.shardingjdbcdemo.model" targetProject="src\main\java">
>
> - <sqlMapGenerator targetPackage="mybatis"  targetProject="src\main\resources">
>
> - <javaClientGenerator type="XMLMAPPER" targetPackage="com.tho.shardingjdbcdemo.dao"  targetProject="src\main\java">
>
> - ```xml
> 	<table schema="shard_order" tableName="t_order_1" domainObjectName="Order" ></table>
> 	```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE generatorConfiguration
        PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN"
        "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd">

<generatorConfiguration>
    <context id="MysqlTables" targetRuntime="MyBatis3">
        <jdbcConnection driverClass="com.mysql.cj.jdbc.Driver"
                        connectionURL="jdbc:mysql://192.168.198.101:3306/shard_order?serverTimezone=Asia/Shanghai&amp;useSSL=false"
                        userId="root"
                        password="123456">
            <property name="nullCatalogMeansCurrent" value="true" />
        </jdbcConnection>

        <javaTypeResolver >
            <property name="forceBigDecimals" value="false" />
        </javaTypeResolver>

        <javaModelGenerator targetPackage="com.tho.shardingjdbcdemo.model" targetProject="src\main\java">
            <property name="enableSubPackages" value="true" />
            <property name="trimStrings" value="true" />
        </javaModelGenerator>

        <sqlMapGenerator targetPackage="mybatis"  targetProject="src\main\resources">
            <property name="enableSubPackages" value="true" />
        </sqlMapGenerator>

        <javaClientGenerator type="XMLMAPPER" targetPackage="com.tho.shardingjdbcdemo.dao"  targetProject="src\main\java">
            <property name="enableSubPackages" value="true" />
        </javaClientGenerator>

        <table schema="shard_order" tableName="t_order_1" domainObjectName="Order" ></table>


    </context>
</generatorConfiguration>
```

## 5.1、sharding-jdbc简介

- Sharing-Jdbc 是一个客户端代理模式,不需要单独启动服务，直接使用 jar 包即可
- 开源 分布式关系型数据库中间件
- 目前已经进入Apache 孵化器
- 定位为轻量级java框架，以jar包提供服务
- 可以理解为增强版 jdbc 驱动
- 完全兼容各种 ORM 框架

![image-20220101152521852](%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8.assets/image-20220101152521852.png)

> 提供了四种配置方式
>
> - Java API ，Yaml，SpringBoot 和 Spring 命名空间



> 与 Mycat 的区别
>
> - Mycat 是服务端代理 Sharding-Jdbc 是客户端代理
> - Mycat 不支持同一库内水平切分，Sharding-Jdbc 支持

## 5.2、Sharing-Jdbc分片表

> 环境配置 两个数据库 
>
> 数据库1 (两个表) 
>
> - t_order_1 
> - t_order_2
>
> 数据库2 (两个表)
>
> - t_order_1
> - t_order_2
>
> 根据 userId 区分不同数据库 (奇偶区分)
>
> 根据 id 区分不同表 (奇偶区分)

## 5.3、SpringBoot整合ShardingJdbc

### 1、maven依赖

```xml
<!-- https://mvnrepository.com/artifact/org.apache.shardingsphere/sharding-jdbc-spring-boot-starter -->
        <dependency>
            <groupId>org.apache.shardingsphere</groupId>
            <artifactId>sharding-jdbc-spring-boot-starter</artifactId>
            <version>4.0.0-RC2</version>
        </dependency>
```

### 2、配置文件配置

```properties
# 配置真实数据源
spring.shardingsphere.datasource.names=ds1,ds2

# 配置第 1 个数据源
spring.shardingsphere.datasource.ds1.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.ds1.driver-class-name=com.mysql.cj.jdbc.Driver
spring.shardingsphere.datasource.ds1.jdbcUrl=jdbc:mysql://192.168.198.101:3306/shard_order
spring.shardingsphere.datasource.ds1.username=root
spring.shardingsphere.datasource.ds1.password=123456

# 配置第 2 个数据源
spring.shardingsphere.datasource.ds2.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.ds2.driver-class-name=com.mysql.cj.jdbc.Driver
spring.shardingsphere.datasource.ds2.jdbcUrl=jdbc:mysql://192.168.198.102:3306/sharding-order
spring.shardingsphere.datasource.ds2.username=root
spring.shardingsphere.datasource.ds2.password=123456

spring.shardingsphere.sharding.tables.t_order.actual-data-nodes=ds$->{1..2}.t_order_$->{1..2}
spring.shardingsphere.sharding.tables.t_order.database-strategy.inline.sharding-column=user_id
spring.shardingsphere.sharding.tables.t_order.database-strategy.inline.algorithm-expression=ds$->{user_id % 2 + 1} # 根据奇偶选择分片时注意数据库和表的名称
spring.shardingsphere.sharding.tables.t_order.table-strategy.inline.sharding-column=id
spring.shardingsphere.sharding.tables.t_order.table-strategy.inline.algorithm-expression=t_order_$->{id % 2 + 1}

mybatis.mapper-locations=/mybatis/*.xml


```



### 3、mapper 文件要修改，要改成逻辑表的名称

> ```java
> <select id="countByExample" parameterType="com.tho.shardingjdbcdemo.model.OrderExample" resultType="java.lang.Long">
>  
>   // 本来是 select count(*) from t_order_1 (由mybatis-generator 生成的mapper) 
>   select count(*) from t_order
>   <if test="_parameter != null">
>     <include refid="Example_Where_Clause" />
>   </if>
> </select>
> ```

### 4、测试

```java
package com.tho.shardingjdbcdemo;

import com.tho.shardingjdbcdemo.dao.OrderMapper;
import com.tho.shardingjdbcdemo.model.Order;
import com.tho.shardingjdbcdemo.model.OrderExample;
import javafx.application.Application;
import org.junit.jupiter.api.Test;
import org.junit.runner.RunWith;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.junit4.SpringRunner;

import java.util.List;

@RunWith(SpringRunner.class)
@SpringBootTest(classes = ShardingJdbcDemoApplication.class)
class ShardingJdbcDemoApplicationTests {

    @Test
    void contextLoads() {
    }

    @Autowired
    private OrderMapper orderMapper;
    @Test
    public void shardingJdbcInsertTest() {
        Order order = new Order();
        order.setId(2);
        order.setUserId(1);
        order.setOrderAmount(2L);
        order.setOrderStatus(2);

        orderMapper.insert(order);
    }
    @Test
    public void shardingJdbcQueryTest() {
        OrderExample orderExample = new OrderExample();
        OrderExample.Criteria criteria = orderExample.createCriteria().andIdEqualTo(2).andUserIdEqualTo(1);

        List<Order> orders = orderMapper.selectByExample(orderExample);
        orders.forEach(o -> System.out.println(o.getId() +  "---"+ o.getUserId()));
    }
}
```

### 5、mybatis-generator生成的通用mapper

```java
// 参数是 对应类的 Example 类 不是 Criteria 类
List<Order> selectByExample(OrderExample example); 
```

## 5.4、ShardingJdbc全局表

> 环境准备
>
> - 各个数据库分片建立 全局表 
> - 配置文件修改 加入下面一行
>
> ```java
> spring.shardingsphere.sharding.broadcast-tables=area
> ```
>
> 

测试类

```java
@Test
public void shardingJdbcGlobalTest() {
    Area area = new Area();
    area.setId(1);
    area.setName("北京");
    areaMapper.insert(area);
}
```

> 启动测试方法，两张表都加入了 插入的数据

## 5.5、ShardingJdbc子表



## 5.6、ShardingJdbc 读写分离

